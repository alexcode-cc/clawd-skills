---
name: long-research
description: >
  [BETA] Deep research that actually reads pages instead of summarizing search results.
  Tell it how long to research (10 min, 2 hours, all night) and it works the full
  duration â€” searching, reading every result, following leads, cracking forums,
  cross-verifying findings, and writing progressively to a research file.
  Tree-style exploration: each page read spawns new searches, like a human researcher.
  Enforced read-to-search ratio prevents shallow search-spamming. Wall-clock time
  commitment â€” it won't finish early. Self-audit gate blocks delivery until quality
  checks pass. Works with web_search, web_fetch, and browser-use for JS-heavy sites.
dependencies:
  - browser-use
---

# Long Research

> âš ï¸ **BETA** â€” This skill works as described but is under active development. Review the notes below before installing.

## Dependencies

- **web_search** â€” any search provider (Perplexity Sonar, Brave, etc.)
- **web_fetch** â€” built into most agent frameworks
- **browser-use** (REQUIRED) â€” install via `pip install browser-use && browser-use install`. Used for JS-heavy sites, login-gated forums, and retailer pricing. The skill will not function fully without it.

## Privacy & Trust Notes

**browser-use remote mode:**
The browser-use cascade tries 3 modes in order: `chromium` (local, free) â†’ `real` (local, free) â†’ `remote` (cloud-hosted, burns API credits). Remote mode sends page content to browser-use.com's cloud infrastructure. If you care about privacy, you can:
- Remove `remote` from the cascade and only use local modes
- Run in Interactive mode so the agent asks before escalating to remote
- The skill works fine with local-only browser modes for most sites

**Login-gated forums and browser profiles:**
The skill includes patterns for extracting content from login-gated forums using `--profile` flags. Profiles persist cookies locally on your machine in browser-use's default profile directory. The agent does NOT attempt automated login â€” it uses browser-use's cookie persistence from previous manual sessions. If you haven't logged into a forum manually via browser-use before, the agent won't have access. No credentials are stored or transmitted by this skill.

**Filesystem writes:**
Research output is written to `research/[topic]-[date].md` in your agent's working directory. Progressive writes happen every 3-5 tool calls as crash recovery. Files stay on your local machine â€” nothing is uploaded. If running in a shared environment, configure your agent's working directory to a safe location.

**Sub-agent prompt injection surface:**
The skill mandates pasting full instructions into sub-agent task prompts. This means the entire SKILL.md (including your research query) is sent to whatever model provider your sub-agent uses. If you use external/remote model providers, be aware that your research queries and the full skill text are transmitted to those services. This is standard for any agent skill that delegates to sub-agents â€” but worth noting if your research topics are sensitive.

**Recommended for first-time users:**
- Run in **Interactive** mode (not Autonomous) until you're comfortable with the skill's behavior
- Start with a short duration (10 min) to see how it works
- Review the research file output before trusting findings

## Activation

When the user invokes this skill (e.g. "do long research", "research X", "pull up long research"), **immediately start the pre-flight checklist**. Do NOT ask "what topic?" separately â€” go straight into the pre-flight questions. If the user already provided a topic in their message, pre-fill it and ask the remaining questions.

## Pre-Flight Checklist (MANDATORY â€” NO EXCEPTIONS)

Before starting ANY research, confirm all of these with the user:

1. **Topic** â€” What exactly to research. Get specific. If not provided yet, ask now.
2. **Duration** â€” How long to spend. Minimum 10 minutes, can be hours.
3. **Autonomy** â€” Choose one. Default: Autonomous.
   - **Autonomous** â€” No questions, log everything, report at end.
   - **Interactive** â€” May ask clarifying questions during research. Pauses for answers.
   - **Interactive-continue** â€” May ask clarifying questions, but if user doesn't reply within ~2 minutes, continue research with best judgment. Never block on silence.
4. **Tools** â€” List in priority order (highest priority first). The plan block MUST reflect this ordering. Default priority:
   1. **web_search** â€” discovery, overviews, forum consensus
   2. **web_fetch** â€” reading specific review/forum/article pages
   3. **browser-use** (REQUIRED dependency) â€” retailer pricing (Amazon, etc.), JS-heavy sites, anything web_fetch can't reach. See `references/browser-use-patterns.md`
   
   User may reorder (e.g., browser-use first for pricing-heavy research). Always include all three in the plan block â€” never omit browser-use.
5. **Scheduling** â€” Now or delayed? If delayed, note time + timezone.
6. **Output** â€” Where to deliver (this chat, specific topic, file only) and format (summary, full report, comparison table).
7. **Clarifying questions** â€” Ask anything about the user's situation BEFORE starting. Don't assume hardware, location, budget, preferences, etc.

â›” **GATE: You MUST post the plan block below and receive explicit user approval before making ANY research tool calls.** Do not start research based on implied approval, partial answers, or "just get going" energy. The plan block IS the gate.

Post this and wait for approval:
```
ğŸ“‹ Research Plan
â€¢ Topic: [what]
â€¢ Duration: [X] minutes/hours
â€¢ Mode: Autonomous / Interactive / Interactive-continue
â€¢ Tools: [list in priority order, e.g. web_search â†’ web_fetch â†’ browser-use]
â€¢ Start: Now / [scheduled time]
â€¢ Output: [where and format]
â€¢ Questions: [anything unclear about user's situation]

Proceed? (yes to start)
```

If the user answers clarifying questions but doesn't say "proceed/yes/go", re-post the updated plan and ask again.

---

## Spawning (CRITICAL â€” read this before delegating to a sub-agent)

â›” **The sub-agent MUST have the full skill instructions in its task prompt.** The root cause of most research failures is the orchestrator paraphrasing the skill instead of injecting it. A sub-agent that doesn't read this file will ignore every gate, every ratio, every enforcement rule.

### How to spawn a research sub-agent:

1. **Read this entire SKILL.md** into context (you're already doing this if you're reading this).
2. **Construct the task prompt** with these MANDATORY sections in this order:

```
TASK PROMPT TEMPLATE (paste in this order â€” critical rules at TOP and BOTTOM):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
## â›” CRITICAL RULES (read first, enforce always)
1. READ > SEARCH. After Seed, you cannot search unless you've read something since your last search.
2. NEVER start synthesis while time remains. Run `date +%s` and check END_TS.
3. browser-use cascade: chromium â†’ real â†’ remote. Try ALL 3 before giving up on any source.
4. web_search returns URLS, not answers. Ignore Perplexity's synthesis text. Extract only the citation URLs.
5. "Not found" IS a valid finding. If you can't find what was asked, say so honestly. Do NOT answer an adjacent question.

## Research Task
[The user's EXACT question, quoted verbatim. Do not paraphrase, soften, or broaden it.]

## Task Anchor (re-read every 5 tool calls)
Your job is to answer THIS question: "[exact question again]"
Every 5 tool calls, ask yourself: "Does my last action directly serve THIS question?"
If no â†’ STOP current branch â†’ re-orient.

## User Context
[Any relevant context â€” location, timeline, constraints. Keep separate from the task.]

## Research Rules
[Paste the FULL Execution section from SKILL.md â€” the complete Read-Driven Loop,
browser-use Cascade, Forum-Cracking Playbook, Progressive Writing, Time Enforcement,
Negative Results, Self-Audit, etc.]

## â›” REMINDER (read last, enforce always)
- You MUST read more pages than you search. Track: reads vs searches.
- You MUST try all 3 browser modes before giving up.
- You MUST NOT start synthesis while time remains.
- You MUST deliver honest negative results, not drift to adjacent questions.
- You MUST run the self-audit before delivering.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

3. **Never summarize the rules.** Paste them. The sub-agent needs the actual enforcement language.
4. **Quote the user's question verbatim** in both Task and Task Anchor. Do not rephrase "find real life experience where X happened" into "research whether X is common."
5. **Put critical rules at both TOP and BOTTOM** of the prompt â€” primacy and recency effects in attention mean middle sections get ignored.

### Common spawning mistakes:
- âŒ "Research X for 10 minutes following the long-research skill" â€” the sub-agent doesn't have the skill
- âŒ Paraphrasing the question â€” changes what the agent optimizes for
- âŒ Adding your own interpretation â€” "Focus on category A" when user said "any category"
- âŒ Omitting browser-use cascade details â€” agent won't know to retry
- âŒ Rules only in the middle of a long prompt â€” agent attention drops off
- âœ… Critical rules at top AND bottom + full rules in middle + exact question quoted

---

## Execution

### Where to Run
- **Dedicated topic or sub-agent** â€” never in main session
- Sub-agent for fully autonomous tasks
- Dedicated topic if interactive (so user can engage)

### The Read-Driven Loop (replaces the old "Execution Flow")

The fundamental architecture: **reading pages is the default action. Searching is only allowed when the URL queue is empty.** This prevents the search-addiction pattern where agents fire 15 searches and read 3 pages.

```
1. SETUP
   â”œâ”€ START_TS=$(date +%s)
   â”œâ”€ END_TS=$((START_TS + DURATION_SECONDS))
   â”œâ”€ SEARCH_COUNT=0, READ_COUNT=0, WRITE_COUNT=0
   â”œâ”€ Create URL_QUEUE (empty list)
   â””â”€ Post: "ğŸš€ Research started. Deadline: $(date -d @$END_TS '+%H:%M:%S')"

2. SURVEY PHASE (first 30% of committed time â€” BREADTH before depth)
   â”œâ”€ Goal: IDENTIFY as many candidates/angles as possible. Do NOT deep-dive yet.
   â”œâ”€ Run 3-5 broad searches with different angles/terminology
   â”œâ”€ For each search: extract citation URLs + candidate names/items mentioned
   â”œâ”€ Build a CANDIDATE LIST: every distinct option/entity mentioned across all sources
   â”œâ”€ â›” GATE: Write #1 to file (Task Anchor + FULL candidate list + URL queue)
   â”œâ”€ CANDIDATE GATE: You MUST have identified â‰¥10 candidates before deep-diving ANY of them.
   â”‚   If topic naturally has fewer (e.g., "best 3 X"), adjust to â‰¥2x the expected answer count.
   â”œâ”€ WRITE_COUNT += 1
   â””â”€ Post: "ğŸ“‹ Survey complete. [N] candidates identified: [list]. Now deep-diving."
   
   Example: Don't stop at the first 3 options you find. If researching [Topic], survey should
   uncover at least 10-20 candidates across different categories, brands, or approaches.
   THEN prioritize which to deep-dive based on relevance to the question.

3. MAIN LOOP (repeat until time runs out)
   â”‚
   â”œâ”€ IF URL_QUEUE is not empty:
   â”‚   â”œâ”€ Pop next URL from queue
   â”‚   â”œâ”€ RELEVANCE CHECK: Is this URL likely to contain what the Task Anchor asks for?
   â”‚   â”‚   If clearly irrelevant (e.g., beginner guide when searching for expert anecdotes): SKIP, log as "pruned: [reason]"
   â”‚   â”œâ”€ READ it (web_fetch first, browser-use cascade if fails)
   â”‚   â”œâ”€ After reading, assess: "Did this page help answer the Task Anchor?" 
   â”‚   â”‚   If yes: READ_COUNT += 1 (productive read)
   â”‚   â”‚   If no: DEAD_END_COUNT += 1 (log as dead end, don't count as productive)
   â”‚   â”œâ”€ Extract: findings + new URLs â†’ add new URLs to queue
   â”‚   â”œâ”€ If read spawns a question that needs a NEW search:
   â”‚   â”‚   â””â”€ ALLOWED (because you just read something)
   â”‚   â”‚       â”œâ”€ web_search â†’ extract citation URLs â†’ add to queue
   â”‚   â”‚       â””â”€ SEARCH_COUNT += 1
   â”‚   â””â”€ Continue to next iteration
   â”‚
   â”œâ”€ IF URL_QUEUE is empty AND you need more:
   â”‚   â”œâ”€ web_search with NEW terms (not repeating old queries)
   â”‚   â”œâ”€ SEARCH_COUNT += 1
   â”‚   â”œâ”€ Extract URLs â†’ add to queue
   â”‚   â””â”€ Go back to reading from queue
   â”‚
   â”œâ”€ Every 3-5 tool calls: write findings to file, WRITE_COUNT += 1
   â”‚
   â”œâ”€ Every 5 tool calls: 
   â”‚   â”œâ”€ Re-read Task Anchor â€” am I still on-target?
   â”‚   â”œâ”€ Run: NOW=$(date +%s); REMAINING=$((END_TS - NOW))
   â”‚   â”œâ”€ Post: â±ï¸ elapsed/committed (remaining) | ğŸ” S | ğŸ“„ R | ğŸ“ W | ğŸª¦ D (dead ends) | R>S? yes/no
   â”‚   â””â”€ Verify: READ_COUNT > SEARCH_COUNT? If not, STOP searching, READ.
   â”‚
   â”œâ”€ â›” BROWSER-USE DEADLINE: By 50% of committed time, you MUST have attempted
   â”‚   at least 1 browser-use call. If you reach the halfway mark without one,
   â”‚   STOP everything and run a browser-use cascade on your most promising blocked URL.
   â”‚
   â”œâ”€ â›” NO IDLE WAITING â€” NEVER USE `sleep` EXCEPT FOR browser-use PAGE LOADS (max 3s):
   â”‚   If you have time remaining, you MUST be making productive tool calls.
   â”‚   Running `sleep 60` or any sleep > 3s to fill time is a HARD FAIL.
   â”‚   
   â”‚   Before you even think about sleeping, run this MANDATORY checklist:
   â”‚   â–¡ Have I tried the top 3 forums for this domain?
   â”‚   â–¡ Have I tried old.reddit.com web_fetch on at least 1 subreddit?
   â”‚   â–¡ Have I tried Google cache on at least 1 blocked URL?
   â”‚   â–¡ Have I searched YouTube comments?
   â”‚   â–¡ Have I searched in non-English languages (if relevant)?
   â”‚   â–¡ Have I tried older threads (2020, 2021, 2022)?
   â”‚   â–¡ Have I tried alternative community forums beyond the obvious ones?
   â”‚   â–¡ Have I cross-verified my existing findings with a second source?
   â”‚   If ANY box is unchecked â†’ DO THAT instead of sleeping.
   â”‚
   â””â”€ TIME CHECK: if $(date +%s) >= END_TS â†’ exit loop, go to SYNTHESIS

4. SYNTHESIS (only after time is up)
   â”œâ”€ â›” GATE: Run date +%s. Print result. If NOW < END_TS, go back to step 3.
   â”œâ”€ Write final version of research file
   â”œâ”€ Include: Task Anchor, Executive Summary, Research Tree, Findings, Sources, Verification
   â””â”€ WRITE_COUNT += 1

5. SELF-AUDIT (hard gate â€” blocks delivery)
   â”œâ”€ Run every checklist item (see Self-Audit section below)
   â”œâ”€ Fix any failures
   â””â”€ Write Process Compliance section

6. DELIVER
   â”œâ”€ Chat summary (<500 words)
   â”œâ”€ Link to research file
   â””â”€ Final elapsed time from date +%s
```

**Why this works better than a ratio rule:**
- The ratio is *architecturally enforced*: you can only search when the queue is empty OR after completing a read.
- Searching is the exception, reading is the default.
- The agent can't chain 5 searches in a row because the loop structure doesn't allow it.
- The READ_COUNT > SEARCH_COUNT check every 5 calls catches any drift.

### The Search Discipline

**web_search returns URLs, not answers.**

When web_search returns a response from Perplexity/Sonar:
1. **Extract ONLY the citation URLs** from the response.
2. **Add those URLs to your queue.**
3. **Ignore the synthesized text.** Do not read it. Do not cite it. Do not base any finding on it.

Why: Perplexity fabricates quotes, hallucinates thread URLs, and presents unverified claims as consensus. If you read its synthesis, you'll unconsciously treat it as a finding. Don't read it. URLs only.

**After Seed phase, you earn each search by reading:**
- âœ… READ â†’ SEARCH â†’ READ â†’ SEARCH (alternating)
- âœ… READ â†’ READ â†’ READ â†’ SEARCH (reading more is always fine)
- âŒ SEARCH â†’ SEARCH (never, unless in Seed)
- âŒ SEARCH â†’ SEARCH â†’ READ (too late, you already chained)

**Hard search cap:** Maximum searches = 1.5x committed minutes. For 10 min = max 15 searches.
If you hit the cap, you are BLOCKED from searching for the rest of the session. Read only.
At any checkpoint: if SEARCH_COUNT > 2x READ_COUNT â†’ BLOCKED from searching until reads catch up.

**PDF URLs:** If a URL ends in `.pdf`, use browser-use to render it. web_fetch returns binary garbage for PDFs. Skip PDFs entirely if browser-use is unavailable.

### Source Balance: User Reports vs Official Research (MANDATORY)

**For grey-area topics** (supplements, nootropics, off-label treatments, emerging tech, anything without large studies or established consensus):
- User reports ARE the data. The person on a forum who's been using a product for 6 months IS a primary source.
- **Hard rule: â‰¥40% of productive reads must be forum/user-report sources** (not academic papers, not marketing sites, not review articles).
- Track: FORUM_READ_COUNT separately from STUDY_READ_COUNT.
- At every 5-call checkpoint: `Forum reads / Total reads >= 0.4?` If not â†’ next reads MUST be forums.
- Forums to prioritize: Reddit (relevant subreddits), StackExchange, specialized community forums for the topic, niche discussion boards, practitioner blogs with comment sections

**For well-studied topics** (established science, mainstream products):
- Official research takes priority. Forum reads still valuable but ratio can be 30/70.

### Adversarial Thinking (MANDATORY â€” prevents confirmation bias)

For EVERY candidate/option the research identifies as "good" or "recommended":
1. **Search for the negative case.** Literally search: `"[candidate] dangers" OR "[candidate] problems" OR "[candidate] risks" OR "[candidate] didn't work"`
2. **Follow the mechanism chain.** If something works by mechanism X, ask: "What ELSE does mechanism X do? Could it cause harm?"
   - Example: If X promotes growth/healing via mechanism Y â†’ Does Y also have unwanted side effects? Search it.
   - Example: If X improves metric A â†’ Does it also worsen metric B? Search it.
   - Example: If X suppresses process P â†’ Does that suppression cause downstream problems? Search it.
3. **Devil's advocate search quota:** At least 1 adversarial search per 3 candidates identified. If you found 9 options, you need â‰¥3 "what could go wrong" searches.
4. **Write a "Risks & Concerns" section** that is at LEAST 25% the length of the "Benefits" section. If your benefits section is 2000 words and risks is 200 words, you're not being honest.

### Task Anchoring (MANDATORY â€” prevents drift)

The #1 failure mode is **task drift** â€” answering an adjacent, easier question.

**Mechanism:**
1. Write the user's **exact question** (verbatim) at the top of the research file under `## Task Anchor`.
2. Every 5 tool calls, re-read it and ask: "Does my last action directly help answer THIS question?"
3. If no â†’ STOP the current branch â†’ re-orient.

**Examples of drift:**
- User asks: "find real life experience where [specific event] happened"
- âŒ Drift: researching general requirements (answers "what's needed" not "who experienced it")
- âŒ Drift: reading official policy documents (answers "what's the rule" not "what happened")
- âœ… On-target: finding a forum thread where someone says "this happened to me: [specific event]"

**The test:** If your findings say "according to official policy..." instead of "user X on forum Y reported that..." â€” you've drifted.

### Negative Results (MANDATORY â€” don't panic-pivot)

Sometimes the answer is: **"I looked hard and didn't find it."**

This is a VALID research outcome. Do NOT:
- âŒ Pivot to answering an adjacent question to have "something to show"
- âŒ Present policy/official guidance as a substitute for the anecdotes you couldn't find
- âŒ Inflate weak leads into findings
- âŒ Quote Perplexity synthesis as if it were a real source

Instead:
- âœ… Document exactly what you searched for and where (queries + forums + pages read)
- âœ… State clearly: "After [N] searches and [M] page reads across [forums], I found zero anecdotes matching [exact question]"
- âœ… Report adjacent findings separately: "While I didn't find X, I did find Y which is related but distinct"
- âœ… Assess WHY you might not have found it: rare event? wrong search terms? content behind paywalls? topic not discussed publicly?
- âœ… Suggest what would need to happen to find it: "This might require searching [specific forum] with a logged-in account" or "This may be too rare for public discussion"

**The summary for a negative result should lead with the negative.** Don't bury it under adjacent findings.

---

## Tool Rules

### General
- **Retailer sites (Amazon, Best Buy, etc.)**: ALWAYS browser-use with `--profile`. web_fetch cannot scrape Amazon.
- **Forums/reviews**: web_fetch first, browser-use cascade if blocked.
- **Discovery**: web_search (for URLs, not answers).
- **browser-use is a hard requirement.** If not installed: `pip install browser-use && browser-use install`.

### browser-use Cascade (try ALL 3 â€” no exceptions)

browser-use has 3 browser modes. You MUST try them **in order**. Do NOT give up after one failure.

**Use this exact bash pattern:**
```bash
URL="https://example.com"
SESSION="research"

# Attempt 1: chromium (free)
echo ">>> Trying chromium..."
browser-use --session $SESSION --browser chromium open "$URL" 2>&1 | tail -5
# If "url:" appears in output â†’ SUCCESS

# Attempt 2: real (free)  
echo ">>> Trying real..."
browser-use --session $SESSION --browser real open "$URL" 2>&1 | tail -5

# Attempt 3: remote (paid, last resort)
echo ">>> Trying remote..."
browser-use --session $SESSION --browser remote open "$URL" 2>&1 | tail -5
```

**Run all 3 in sequence. Stop at the first success. Log all attempts:**
```
[BROWSER CASCADE: {url}]
  chromium: SUCCESS/FAIL (reason)
  real: SUCCESS/FAIL (reason)
  remote: SUCCESS/FAIL (reason)
```

â›” If you get an **import error** (e.g., `BrowserConfig` not found), that's a code bug, not a site block. Fix the code (use `BrowserProfile`) and retry. Do NOT count code bugs as cascade failures.

â›” NEVER give up after 1 failure. NEVER skip to "find alternative source" without trying all 3 modes. If you log only 1 attempt, the self-audit will fail you.

### web_fetch Failure Escalation
When web_fetch returns 403, empty content, Cloudflare, or broken HTML:
1. Run the **full browser-use cascade** (all 3 modes)
2. Log: `[web_fetch BLOCKED: {url} â†’ cascade: chromium=X, real=Y, remote=Z]`
3. Only after all 3 fail: find an alternative source (but log that you tried)

### Forum-Cracking Playbook

Forums are where real experiences live. They're also the hardest to access. Use these strategies IN ORDER:

**Reddit (NOTE: as of 2026, Reddit blocks most automated access including old.reddit.com):**
1. `old.reddit.com` via web_fetch (try first but expect 403)
2. Google cache: web_search `cache:reddit.com/r/[sub]/comments/[id]`
3. Reddit aggregation sites that compile Reddit reviews by topic
4. browser-use cascade on `reddit.com` (last resort)
5. web_search `site:reddit.com "[exact phrase]"` â€” use citation URLs, NOT Perplexity synthesis
6. **Fallback forums when Reddit fails:** StackExchange, specialized community forums for the topic, niche discussion boards

**Login-gated forums:**
1. web_fetch (sometimes works for newer threads)
2. browser-use chromium + JavaScript eval to extract post content:
   ```bash
   browser-use --session forum --browser chromium open "https://forum.example.com/thread/[ID]"
   # Wait 2s for page load
   browser-use --session forum --browser chromium eval "Array.from(document.querySelectorAll('[data-role=commentContent], .post-body, .message-content')).map((e,i) => 'POST ' + i + ': ' + e.innerText.substring(0,600)).join('\n===\n')"
   ```
3. This eval pattern extracts post content from login-gated pages â€” adapt selectors per forum.

**Blocked forums:**
1. web_fetch (may fail â€” if so, skip to browser-use)
2. browser-use cascade
3. Google cache

**YouTube comments:**
1. web_fetch on the video page (gets description + some comments)
2. browser-use for full comment sections

**Domain-specific community forums:**
1. Identify the top 3-5 forums for the research topic via web_search
2. web_fetch with /threads/ or /topic/ URLs, search via `site:[forum-domain]`
3. browser-use chromium for search, eval to extract post content
4. Look for practitioner blogs, review aggregation sites, and niche wikis
5. Academic databases for study abstracts when relevant (web_fetch works for most)

**Vendor research with browser-use:**
- Use browser-use eval to extract product catalogs with prices
- Example: `document.querySelectorAll('.product-miniature')` â†’ map name + price
- Always capture full catalog, not just target products

**General fallback for any blocked source:**
1. Google `cache:[url]` via web_fetch
2. Wayback Machine: `https://web.archive.org/web/[url]`
3. Google `site:[domain] "[exact phrase]"` to find alternative pages on the same site

### Anecdote-Hunting Searches (for "find real experiences" tasks)

When the user asks for **real-life experiences**, first-person accounts, or anecdotes:

**Use personal-language queries:**
```
"my experience with" [topic] site:[relevant-forum]
"I tried" OR "I switched to" OR "after using" [topic] forum
"my experience" [topic] "what happened" forum
"I had to" OR "they told me" OR "turns out" [topic keywords]
```

**DO NOT use policy-language queries:**
```
"[topic] requirements overview" â† returns official pages, not stories
"what is needed for [topic]" â† returns guides, not experiences
```

**Source priority for anecdotes:**
1. Specialized forums for the topic (use browser-use eval pattern if login-gated)
2. Reddit (old.reddit.com â†’ Google cache â†’ browser-use)
3. Niche community forums and discussion boards
4. Q&A sites (real expert answers to real questions)
5. YouTube comments
6. Google `site:` searches with first-person language

**The test:** Your findings should contain direct quotes from real people, with usernames and thread URLs. If your report says "typically X happens..." instead of "user @JohnDoe42 on [Forum] reported: '...'" â€” you've answered a different question.

---

## Source Verification (MANDATORY)

- **Never trust Perplexity/Sonar synthesis.** Extract URLs only. Ignore the text.
- When Sonar claims "Reddit users say X": treat it as a LEAD to find URLs, not a source.
- Real user quotes must come from pages you actually fetched.
- If you can't verify a forum claim: mark it `[âš ï¸ unverified forum consensus]`
- â›” **FIREWALL:** Unverified claims CANNOT appear in Recommendation or Executive Summary. They can appear in Detailed Findings with âš ï¸ tag only.

---

## Progressive Writing (MANDATORY â€” even for short sessions)

â›” **GATE: First write MUST happen after Seed phase.** You CANNOT proceed to tree traversal until you've written initial findings, URL queue, and Task Anchor to the research file.

- **Write #1 (GATE):** After Seed â€” Task Anchor, URL queue, search terms. No more tool calls until this write lands.
- **Subsequent writes:** Every 3-5 tool calls â€” append new findings in batches.
- **Final write:** Synthesis phase â€” executive summary, tree, recommendation.
- â›” **NEVER accumulate all findings and write once at the end.** If session crashes, all data is lost.
- **â‰¤15 min sessions:** minimum 3 writes. **>15 min:** minimum 1 write per 5 minutes.
- **Count writes.** Every status update: `ğŸ“ [N] file writes`. If N=0 after 4+ tool calls â†’ STOP and write NOW.

---

## Time Enforcement (WALL-CLOCK â€” NO GUESSING)

**Setup:**
```bash
START_TS=$(date +%s)
END_TS=$((START_TS + DURATION_SECONDS))
echo "START_TS=$START_TS" > /tmp/research_time
echo "END_TS=$END_TS" >> /tmp/research_time
echo "Deadline: $(date -d @$END_TS '+%H:%M:%S UTC')"
```

**Every status check:**
```bash
source /tmp/research_time
NOW=$(date +%s)
echo "Elapsed: $(( (NOW - START_TS) / 60 )) min | Remaining: $(( (END_TS - NOW) / 60 )) min"
```

**Status format:**
```
â±ï¸ [elapsed] / [committed] ([remaining] left) | ğŸ” [S] searches | ğŸ“„ [R] reads | ğŸ“ [W] writes | R>S? [yes/no]
```

The `R>S?` field: reads must exceed searches. If it says "no", stop searching and read.

### Synthesis Gate (HARD â€” prevents finishing early)

â›” **Before starting synthesis, you MUST run this check:**
```bash
source /tmp/research_time
NOW=$(date +%s)
if [ $NOW -lt $END_TS ]; then
  echo "BLOCKED: $((( END_TS - NOW ) / 60)) minutes remaining. Go back to research."
else
  echo "TIME UP: Proceed to synthesis."
fi
```

**If the check prints "BLOCKED":** You CANNOT synthesize. Go back to the Main Loop. Use the remaining time for:
- New search terms (synonyms, adjacent topics, different languages)
- Older threads (add year ranges: "2020", "2021", etc.)
- Adjacent communities (different forums)
- Cross-verifying existing findings with additional sources
- Following up on leads you deprioritized earlier
- YouTube comments on relevant videos

**If the check prints "TIME UP":** Proceed to synthesis.

---

## Context Management (see `references/context-management.md`)
- Write findings to `research/[topic]-[date].md` progressively.
- Chat context gets summaries only â€” detail goes to file.
- Write checkpoints every 5-10 minutes (recovery anchors after compaction).
- Long sessions WILL compact â€” that's expected and fine.
- Never stop research due to context size.

---

## Pricing Data (when applicable)
If the research involves products/purchases:
- **MUST hit at least one retailer** via browser-use for real pricing.
- web_search price summaries are estimates, NOT verified.
- Note currency, tax inclusion (e.g., regional pricing differences such as VAT inclusion vs exclusion), date of check.

---

## Output

### Research File (`research/[topic]-[date].md`)
```markdown
# [Topic] Research
**Date:** [timestamp]
**Duration:** [actual time spent]
**Tools used:** [list]
**Searches:** [count] | **Deep reads:** [count] | **Ratio R:S:** [ratio]

## Task Anchor
> [User's exact question, quoted verbatim]

## Executive Summary
[3-5 bullet points â€” ONLY âœ… verified findings]
[If negative result: lead with "After X searches and Y page reads, no [specific thing] was found."]

## Research Tree
[SEARCH â†’ READ â†’ spawned â†’ READ â†’ etc. Shows traversal.]

## Detailed Findings
### [Category 1]
[findings with source URLs, âœ…/âš ï¸ markers]

## Comparison Table (if applicable)

## Sources
[numbered list of URLs actually fetched â€” NOT search result URLs]

## Source Verification
[âœ… verified (fetched actual page) | âš ï¸ unverified (search synthesis) | ğŸ”— direct quote]

## Confidence Notes
[what's solid vs uncertain]

## Recommendation
[ONLY âœ… verified sources. If negative result, recommend next steps to find the answer.]

## Process Compliance
[Self-audit pass/fail for each gate + remediation taken]
```

### Chat Summary
Post concise summary (<500 words) when done. Link to full research file.

---

## Post-Research Self-Audit (HARD GATE â€” blocks delivery)

â›” **You CANNOT post the chat summary until every box is checked.** Run the audit, fix failures, THEN deliver.

### Process Gates
- [ ] START_TS and END_TS set with `date +%s`
- [ ] All time reports used real `date +%s` (no guessing)
- [ ] Time commitment honored â€” verify: final `date +%s` â‰¥ END_TS
- [ ] Synthesis Gate bash check was run and printed "TIME UP"
- [ ] Task Anchor written to file and re-checked every 5 tool calls
- [ ] **No idle waiting** â€” no sleep loops or timer countdowns. Every minute had active tool calls.

### Read/Search Balance
- [ ] READ_COUNT > SEARCH_COUNT â€” post actual numbers: `reads: [R] / searches: [S]`
- [ ] No SEARCHâ†’SEARCH chains outside Seed phase
- [ ] Every search after Seed was preceded by at least 1 read
- [ ] **Dead ends tracked** â€” reads on irrelevant pages logged as dead ends, not counted as productive reads. Post: `ğŸª¦ [D] dead ends`

### Writing Gates
- [ ] Write #1 happened after Seed (before tree traversal)
- [ ] â‰¥3 writes for â‰¤15 min sessions, or 1/5min for longer
- [ ] Research tree logged in file
- [ ] Every status update included `ğŸ“ [N]` and `R>S? [yes/no]`

### Tool Usage Gates (EVIDENCE REQUIRED â€” not honor system)
â›” **For each browser-use gate, you must paste the actual command and its output.** Checking a box without evidence is audit fraud.

- [ ] browser-use cascade attempted â€” **PASTE the 3 commands and outputs below:**
  ```
  chromium attempt: [paste command + result]
  real attempt: [paste command + result]  
  remote attempt: [paste command + result]
  ```
- [ ] browser-use attempted before 50% of committed time (paste timestamp proof)
- [ ] Every web_fetch failure escalated to full cascade â€” **list each 403 and what you did:**
  ```
  [url] â†’ 403 â†’ cascade: chromium=[result], real=[result], remote=[result]
  ```
- [ ] Reddit 403 â†’ tried old.reddit.com first (paste the web_fetch attempt), then cascade
- [ ] At least 1 successful browser-use interaction in the session

### Source Quality Gates
- [ ] Zero Perplexity synthesis text in Executive Summary or Recommendation
- [ ] All sources have URLs from pages actually fetched
- [ ] Claims marked âœ… verified / âš ï¸ unverified â€” none unmarked
- [ ] Recommendations backed ONLY by âœ… verified sources

### Task Fidelity Gate
- [ ] Re-read Task Anchor. Does the Executive Summary directly answer the exact question?
- [ ] If user asked for anecdotes: report contains direct quotes with usernames + thread URLs
- [ ] If mostly policy/guidance when anecdotes were asked: FAIL â†’ go find stories
- [ ] If negative result: summary LEADS with the negative, doesn't bury it under adjacent findings

### Read Quality Gate
- [ ] Count reads that actually helped answer the Task Anchor vs dead-end reads
- [ ] At least 50% of reads must be on-topic (relevant to the actual question)
- [ ] Dead ends logged with reason: `ğŸª¦ [url] â€” [why irrelevant]`
- [ ] URL queue was filtered for relevance before reading (didn't blindly read every Perplexity citation)

### Survey Breadth Gate
- [ ] Survey Phase completed in first 30% of time
- [ ] Candidate list has â‰¥10 items (or â‰¥2x expected answer count for smaller topics)
- [ ] Candidates written to file BEFORE deep-diving any single one
- [ ] Deep-dive prioritization justified (why these candidates over others)

### Source Balance Gate (for grey-area topics)
- [ ] FORUM_READ_COUNT / total reads â‰¥ 0.4 (40% user reports)
- [ ] Post actual counts: `Forum reads: [F] / Study reads: [S] / Ratio: [F/(F+S)]`
- [ ] Real user quotes with usernames included in findings
- [ ] If ratio < 0.4: explain why (e.g., "no forums exist for this topic")

### Adversarial Thinking Gate
- [ ] At least 1 adversarial search per 3 candidates (e.g., 9 candidates = â‰¥3 "dangers/risks" searches)
- [ ] Each recommended candidate has a "Risks & Concerns" subsection
- [ ] Mechanism chains explored (if X promotes Y for benefit, does Y also cause Z harm?)
- [ ] Risks section is â‰¥25% the length of Benefits section
- [ ] Post: `Devil's advocate searches: [N] / Candidates: [M] / Ratio: [N/ceil(M/3)]`

### Remediation (fix before delivering)
| Failure | Fix |
|---------|-----|
| READ_COUNT â‰¤ SEARCH_COUNT | Read more pages now |
| Missing writes | Write current findings immediately |
| Unverified claims in recommendation | Remove or verify them |
| Time not honored | Continue researching |
| browser-use cascade incomplete | Run remaining modes now |
| Task drift | Re-search with original question terms |
| No anecdotes when requested | Forum searches with first-person language |
| Only 1-2 browser modes tried | Try the remaining modes on any pending source |

---

## Anti-Patterns

### Architecture violations
- âŒ **Search addiction** â€” firing web_search to "see what comes up" instead of reading pages in the queue. The queue is your job. Search only when it's empty or a read spawns a new question.
- âŒ **Reading Perplexity synthesis** â€” the synthesized text from web_search is noise. Extract URLs. Ignore text. If your findings cite "according to Perplexity..." you've failed.
- âŒ **SEARCHâ†’SEARCH chains** after Seed â€” architecturally forbidden. Each search must be earned by a read.
- âŒ **Finishing early** â€” the Synthesis Gate bash check prevents this. Run it. **TIME IS A HARD CONTRACT.** If committed to 10 minutes, you work for 10 minutes. Finishing at 4 min on a 10-min commitment = CRITICAL FAILURE. If you think you're "done" early, you're NOT â€” run the mandatory checklist, search deeper, cross-verify, find more threads. Check `date +%s` against END_TS before ANY final output.
- âŒ **Idle waiting / sleep commands** â€” NEVER use `sleep` for more than 3 seconds (browser-use page loads only). Running `sleep 60` to fill remaining time is a HARD FAIL that the audit WILL catch. The checklist of "things to do when done" is mandatory â€” run through it before claiming you're out of options.
- âŒ **Garbage reads** â€” reading irrelevant pages just to inflate READ_COUNT. A beginner tutorial doesn't count as a "read" when researching expert-level anecdotes. Filter the URL queue for relevance BEFORE reading. Dead-end reads must be logged as dead ends, not productive reads.
- âŒ **Audit fraud** â€” checking self-audit boxes for things you didn't do. If the audit says "browser-use cascade attempted" but you made zero browser-use calls, that's fabrication. The audit requires EVIDENCE (paste commands + outputs), not just checkboxes.

### Content violations
- âŒ **Answering a different question** â€” if user wants anecdotes, don't deliver policy summaries. Re-read the Task Anchor.
- âŒ **Panic-pivoting on negative results** â€” "not found" is valid. Don't inflate weak leads.
- âŒ **Unverified claims in recommendations** â€” if you didn't fetch the page, it can't go in the recommendation.
- âŒ **Marketing claims as benchmarks** â€” distinguish clearly.
- âŒ **Premature narrowing** â€” picking 3-5 candidates in Seed and ignoring everything else. The Survey Phase exists to prevent this. You MUST cast a wide net before deep-diving. If you're deep-diving a candidate before identifying â‰¥10 options, you've narrowed too early.
- âŒ **All-research, no-users** â€” citing 10 academic papers and 0 forum threads. For grey-area topics, user reports are EQUALLY valuable. The 40% forum-read quota exists for a reason.
- âŒ **Confirmation bias** â€” only searching for why something works, never for why it might harm. The Adversarial Thinking section is mandatory. Every "this is great" needs a "but here's what could go wrong" search.

### Tool violations
- âŒ **Giving up after 1 browser-use failure** â€” try ALL 3 modes. Log all attempts.
- âŒ **Silently skipping blocked sources** â€” web_fetch 403 â†’ full cascade, always.
- âŒ **web_fetch on Amazon** â€” use browser-use with `--profile`.
- âŒ **Zero browser-use calls** â€” must attempt at least once per session.

### Process violations
- âŒ **Running in main session** â€” always dedicated topic or sub-agent.
- âŒ **Spawning sub-agent without full rules** â€” paraphrasing = agent ignores rules. Paste them.
- âŒ **Paraphrasing user's question** â€” quote verbatim. "Find real experiences" â‰  "research whether common".
- âŒ **Single write at end** â€” progressive writes mandatory. Session crashes lose everything.
- âŒ **Fabricating elapsed time** â€” every timestamp from `date +%s`. No guessing.
- âŒ **Skipping self-audit** â€” hard gate. No exceptions.
- âŒ **Skipping pre-flight** â€” plan block must be posted and approved.
